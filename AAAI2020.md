#AAAI2020_DRL

- 创新模式总结：
    - （1）将某个问题首次使用强化学习进行建模【4、5、13、14、16、18、19、24、25、26、41、42】
    - （2）将某个已经使用了RL方法的应用或理论问题进行问题场景细分（针对细分场景的问题开展研究）【2、3、6、7、8、11、15、17、21、32、38、39、40、47、48】
    - （3）老问题首次引入某种新技术或提出新方法或改进【9、10、12、22、23、27、28、29、30、31、44、45、51】
    - （4）针对老问题给出理论研究结果，例如bound【20】

- 技术类型总结：
    - （1）有模型 and 无模型 
    - （2） 单agent and 多agent 
    - （3）层次的 
    - （4）确定性 and 不确定性  
    - （5）元强化学习 
    - （6）小规模 and 大规模 
    - （7）迁移适应能力

- 技术类型总结（多智能体）：
    - （1）中心化 and 去中心化 
    - （2）合作
    - （3）通信

- 理论关键词：模拟器【1】、专家信息展示【2】、大规模【3、17】、稳定性鲁棒性不确定性【8、21、39、40、48、49、51】、元强化学习【9】、高效探索采样【51】、监督和强化混合模型、对抗攻击【16、24、29】、逐步分层解决【4、14、44、45】、序列生成【5、49】、复杂对抗博弈【6】、多智能体合作【7、11、12】、模仿强化学习【10】、多智能体博弈【15】、算法理论研究【20】、动作空间裁剪【23】、迁移强化学习【26、31】、通用游戏agent【28】、非马尔科夫奖励【41】、图划分【46】

- 应用关键词：图上组合优化【22】、交通灯【9、17】、智能操盘交易【10、27(预测)】、图像编辑【13】、人类姿态评估【18】、事件摘要【19、45】、句子配对【25】、基于文本的游戏【30】、图像处理【32】、对话生成【44】、基于DAG图的资源分配【46】、自然语言生成【49】、恶意软件检测【47】


**[1]. Google Research Football: A Novel Reinforcement Learning Environment**
- 作者: Karol Kurach (Google Brain)*; Anton Raichuk (Google); Piotr Stańczyk (Google Brain); Michał Zając (Google Brain); Olivier Bachem (Google Brain); Lasse Espeholt (DeepMind); Carlos Riquelme (Google Brain); Damien Vincent (Google Brain); Marcin Michalski (Google); Olivier Bousquet (Google); Sylvain Gelly (Google Brain)
- 简介: 提供了一个新的模拟器，解决当前模拟器对于RL研究的局限性
- 要点: 现有的很多强化学习研究环境存在一些缺点，要么太容易，要么消耗资源太大、要么缺少随机性、要么不开源、要么是已知环境模型的、还有很多游戏只有单agent

**[2]. Reinforcement Learning from Imperfect Demonstrations under Soft Expert Guidance**
- 作者: Xiaojian Ma (University of California, Los Angeles)*; Mingxuan Jing (Tsinghua University); Wenbing Huang (Tsinghua University); Chao Yang (Tsinghua University); Fuchun Sun (Tsinghua); Huaping Liu (Tsinghua University); Bin Fang (Tsinghua University)
- 简介: 为了提高探索效率，可以接入专家示范，但是真实环境下往往是不完美展示，本文提出一种专家展示引导的软方法解决思路，进一步基于该解决思路可以把问题建模为限制优化问题，而对于大规模和高维问题并不好解决，针对这个难点本文进一步提出了一种对偶求解算法。

**[3]. Proximal Distilled Evolutionary Reinforcement Learning**
- 作者: Cristian Bodnar (University of Cambridge)*; Ben Day (University of Cambridge); Pietro Lió (University of Cambridge)
- 简介：解决演化强化学习的scale的问题

**[4]. Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video**
- 作者: Jie Wu (Sun Yat-sen University)*; Guanbin Li (Sun Yat-­sen University); si liu (Beihang University); Liang Lin (DarkMatter AI)
- 简介：针对Temporally language grounding任务，提出了一种利用从粗到细的决策模式的树形结构RL算法，有粗粒度的行动，如整体左移右移尺度变换，细粒度级的操作则是修改左右边界，这样就构成一个树形的策略，相当于有一些宏策略，叶子策略与环境互动获得奖励，根策略不与环境直接互动

**[5]. RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement Learning**
- 作者: Nan Jiang (Tsinghua University)*; Sheng Jin (Tsinghua University); Zhiyao Duan (Unversity of Rochester); Changshui Zhang (Tsinghua University)
- 简介：这篇文章将在线伴奏生成问题建模为序列决策行为

**[6]. Mastering Complex Control in MOBA Games with Deep Reinforcement Learning**
- 作者: Deheng Ye (Tencent)*; Zhao Liu (Tencent); Mingfei Sun (Tencent); Bei Shi (Tencent AI Lab); Peilin Zhao (Tencent AI Lab); Hao Wu (Tencent); Hongsheng Yu (Tencent); Shaojie Yang (Tencent); Xipeng Wu (Tencent); Qingwei Guo (Tsinghua University); Qiaobo Chen (Tencent); Yinyuting Yin (Tencent); Hao Zhang (Tencent); Tengfei Shi (Tencent); Liang Wang (Tencent); Qiang Fu (Tencent AI Lab); Wei Yang (Tencent AI Lab); Lanxiao Huang (Tencent)
- 简介：本文针对王者荣耀1v1任务，提出了一套智能系统，并在其中设计了一系列解决复杂控制问题的方法，解决大规模高效探索的问题。

**[7]. Partner Selection for the Emergence of Cooperation in Multi‐Agent Systems using Reinforcement Learning**
- 作者: Nicolas Anastassacos (The Alan Turing Institute)*; Steve Hailes (University College London); Mirco Musolesi (UCL)
- 简介：研究一种partner selection机制来考虑对方的行为历史和名誉，在多agent去中心化的场景下让合作行为自动涌现，体现了同伴选择对合作涌现的影响

**[8]. Uncertainty-Aware Action Advising for Deep Reinforcement Learning Agents**
- 作者: Felipe Leno da Silva (University of Sao Paulo)*; Pablo Hernandez-Leal (Borealis AI); Bilal Kartal (Borealis AI); Matthew Taylor (Borealis AI)
- 简介：当agent在实际环境中训练的时候，由于对环境不熟悉可能导致agent的毁坏，为了解决这个训练过程中的问题，本文通过建模不确定性，让agent面对不太熟悉的环境的时候可以更加谨慎，将不确定性建模为代价的一部分

**[9]. MetaLight: Value-based Meta-reinforcement Learning for Traffic Signal Control**
- 作者: Xinshi Zang (Shanghai Jiao Tong University)*; Huaxiu Yao (Pennsylvania State University); Guanjie Zheng (Pennsylvania State University); Nan Xu (University of Southern California); Kai Xu (Shanghai Tianrang Intelligent Technology Co., Ltd); Zhenhui (Jessie) Li (Penn State University)
- 简介：针对交通灯控制存在的需要的训练数据量大、计算资源消耗大的问题，提出使用元强化学习的思路，进一步提出了使用元强化学习存在的两个挑战，并提出了具体的解决方案，取得了不错的效果

**[10].Adaptive Quantitative Trading: an Imitative Deep Reinforcement Learning Approach**
- 作者: Yang Liu (University of Science and Technology of China)*; Qi Liu (" University of Science and Technology of China, China"); Hongke Zhao (Tianjin University); Zhen Pan (University of Science and Technology of China); Chuanren Liu (The University of Tennessee Knoxville)
- 简介：论文针对金融量化交易当中的操作频率高、数据噪声大、难以进行探索和利用平衡的问题，提出了将问题建模为POMDP来解决噪声问题，进而利用RDPG来解POMDP问题，为了进一步提升探索和利用效率，加入了经验展示buffer，为了提升行动的连续性采用了行为克隆

**[11]. Neighborhood Cognition Consistent Multi‐Agent Reinforcement Learning**
- 作者: Hangyu Mao (Peking University)*; Wulong Liu (Huawei Noah's Ark Lab); Jianye Hao (Tianjin University); Jun Luo (Huawei Technologies Canada Co. Ltd.); Dong Li ( Huawei Noah's Ark Lab); Zhengchao Zhang (Peking University); Jun Wang (UCL); Zhen Xiao (Peking University)
- 简介：论文考虑了环境一致性认知对多智能体合作的重要性以及在邻居范围内考虑认知一致性的有效性，提出了邻居认知一致性，这个可以和已有MARL方法相结合

**[12]. SMIX(): Enhancing Centralized Value Functions for Cooperative Multi-Agent Reinforcement Learning**
- 作者: Chao Wen (Nanjing University of Aeronautics and Astronautics)*; Xinghu Yao (Nanjing University of Aeronautics and Astronautics); Yuhui Wang (Nanjing University of Aeronautics and Astronautics, China); Xiaoyang Tan (Nanjing University of Aeronautics and Astronautics, China)
- 简介：本文研究的是在多agent问题中的中心值函数设计，在避免维数爆炸的同时具有Q（lamda）的收敛性

**[13]. Unpaired Image Enhancement Featuring Reinforcement-­Learning-Controlled Image Editing Software**
- 作者: Satoshi Kosugi (The University of Tokyo)*; Toshihiko Yamasaki (The University of Tokyo)
- 简介:提出了一种非配对图像增强技术，将photoshop结合到GAN当中。

**[14]. Crowdfunding Dynamics Tracking: A Reinforcement Learning Approach**
- 作者: Jun Wang (University of Science and Technology of China)*; Hefu Zhang (University of Science and Technology of China); Qi Liu (" University of Science and Technology of China, China"); Zhen Pan (University of Science and Technology of China); Hanqing Tao (University of Science and Technology of China (USTC))
- 引用：[AAAI 2017] The option-critic architecture
- 简介：针对基金众筹动态跟踪任务，在option框架下提出一种改进的层次强化学习算法来解决这个问题

**[15]. Model and Reinforcement Learning for Markov Games with Risk Preferences**
- 作者: Wenjie Huang (Shenzhen Research Institute of Big Data)*; Hai Pham Viet (Department of Computer Science, School of Computing, National University of Singapore); William Benjamin Haskell (Supply Chain and Operations Management Area, Krannert School of Management, Purdue University)
- 简介：针对风险偏好时间一致下的马尔科夫多智能体博弈问题，提出了一种近似求解均衡点的q-learning算法

**[16]. Finding Needles in a Moving Haystack: Prioritizing Alerts with Adversarial Reinforcement Learning**
- 作者: Liang Tong (Washington University in Saint Louis)*; Aron Laszka (University of Houston); Chao Yan (Vanderbilt UNIVERSITY); Ning Zhang (Washington University in St. Louis); Yevgeniy Vorobeychik (Washington University in St. Louis)
- 简介：本文的背景是入侵检测问题，当前的方法大多存在false positive太高的问题，最多进一步结合各种启发式的方法来控制这个问题，但是就算通过alert correction减少一下alert集合，量仍然非常大，因此本文提出了利用对抗强化学习框架来解决将问题建模为攻防博弈模型后模型求解计算复杂性太高的问题，最终可以先做alert correction，然后进一步利用agent来选择优先进行探查处理的alert，对抗学习框架中构建一个强大的攻击者，然后进行如下两步的迭代（1）在防守方和攻击方各自对手采用某种确定的随机策略的基础上学习最优策略，（2）扩充各自的最优策略集合

**[17]. Toward A Thousand Lights: Decentralized Deep Reinforcement Learning for Large‐Scale Traffic Signal Control**
- 作者: Chacha Chen (Pennsylvania State University)*; Hua Wei (Pennsylvania State University); Nan Xu (University of Southern California); Guanjie Zheng (Pennsylvania State University); Ming Yang (Shanghai Tianrang Intelligent Technology Co., Ltd); Yuanhao Xiong (Zhejiang University); Kai Xu (Shanghai Tianrang Intelligent Technology Co., Ltd); Zhenhui (Jessie) Li (Penn State University)
- 简介：本文背景是要解决城市级的大规模的交通信号灯控制问题，针对当前方法存在的规模上不去、缺乏全局协调、数据在实际情况下难获取三个方面难以同时满足的问题，借鉴了FRAP的去中心化参数共享、PressLight的pressure概念指标，解决了前两个问题，利用排队长度解决了第三个数据获取的问题，形成了最终的解决方案，首次给出了城市级上千交通灯全局控制问题的解决方案

**[18]. Deep Reinforcement Learning for Active Human Pose Estimation**
- 作者: Erik Gärtner (Lund University)*; Aleksis Pirinen (Lund University); Cristian Sminchisescu (Lund University)
- 简介：论文针对人类姿态评估问题展开研究，提出当前的方法将视角选择和评估分割为两个阶段，视角选择通常由人工完成，这就回带来一些不同观点，有些人是越多越好，有些人是极简主义，有些人是务实主意（有什么用什么），但是没有任何方法将主动观察考虑到模型范围内，本文利用DRL提出了一种将两个部分一起用模型考虑的方法，取得了不错的效果

**[19]. Be Relevant, Non‐redundant, Timely: Deep Reinforcement Learning for Real‐time Event Summarization**
- 作者: Min Yang ( Chinese Academy of Sciences)*; Chengming Li (Chinese Academy of Sciences); Fei Sun (Alibaba Group); Zhou Zhao (Zhejiang University); Ying Shen (Peking University Shenzhen Graduate School); Chenglin Wu (fuzhi.ai)
- 简介：这篇文章主要针对Real-time event summarization当前存在的三个主要挑战，提出一种新的模型有针对性的解决三个问题。文章的书写结构很清晰、连词很丰富，读起来比较容易。

**[20]. A Tale of Two‐Timescale Reinforcement Learning with the Tightest Finite‐Time Bound**
- 作者:Gal Dalal (Technion)*; Balazs Szorenyi (Yahoo Research); Gugan Thoppe (Duke University)
- 简介：这篇完全是一个理论方面的工作，针对GTD类型的RL方法属于的线性双时间尺度随机近似算法，给出了收敛率边界

**[21]. Reinforcement Learning with Perturbed Rewards**
- 作者:Jingkang Wang (University of Toronto); Yang Liu (UCSC); Bo Li (University of Illinois at Urbana–Champaign)*
- 简介：这篇文章针对实际情况中奖励函数收到随机性干扰的问题，针对一种特定的噪声模式，提出了一种噪声评估模型，并对给出的方法进行了收敛性和采样复杂性分析，实验结果表现出了接近在真实奖励上训练得到的性能效果，有时甚至能够超过，原因可能是这种噪声加入以及去噪可能相当于多了一层探索。

**[22]. Exploratory Combinatorial Optimization with Reinforcement Learning**
- 作者: Thomas Barrett (University of Oxford)*; William Clements (Unchartech); Jakob Foerster (Facebook AI Research); Alexander Lvovsky (Oxford University)
- 简介：这篇文章针对图上的组合优化问题，提出一种具有一定通用性的将图神经网络与强化学习算法相结合的工作，由于一般的方法难以得到一个单一的最优解，因此论文认为应该允许agent在测试时探索更好的解。

**[23]. Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction**
- 作者: Vishal Jain (Mila, McGill University)*; Liam Fedus (Google); Hugo Larochelle (Google); Doina Precup (McGill University); Marc G. Bellemare (Google Brain)
- 简介：文章以交互式文字冒险游戏为研究背景，指出该问题的主要挑战是行动空间大、状态部分可观测、奖励稀疏，在前人工作的基础上，该论文分别在奖励和动作两个方面进行了改进，奖励方面提出了分段奖励函数来对不同子任务学习分别的价值函数，针对动作空间大的问题，提出一个辅助可采纳动作预测模块，对动作空间进行了有效的缩小

**[24]. Spatiotemporally Constrained Action Space Attacks on Deep Reinforcement Learning Agents**
- 作者:Xian Yeow Lee (Iowa State University)*; Sambit Ghadai (Iowa State University); Kai Liang Tan (Iowa State University); Chinmay Hegde (New York University); Soumik Sarkar (Iowa State University)
- 简介：论文针对DRL agent面临的对抗攻击问题，重点研究之前被忽略的行动空间攻击问题，主要贡献包括将一个白盒短时行动空间攻击问题表示为一个解耦约束的优化问题，进一步将上述模型延伸为耦合约束来解决非短视行动空间攻击，并提出了一个白盒的有预见性的行动空间攻击策略，实验表明使用同样的预算取得了更强的攻击，然后展示了如何使用这些攻击策略更好的理解agent的脆弱性

**[25]. Modelling Sentence Pairs via Reinforcement Learning: An Actor‐Critic Approach to Learn the Irrelevant Words**
- 作者: MAHTAB AHMED (The University of Western Ontario)*; Robert Mercer (The University of Western Ontario)
- 简介：论文主要针对当前现有句子配对方法存在的没有有效同时考虑一个句子对共享信息的问题，提出一种RL方法来同时考虑两个句子过滤掉不相关词，从而更好地对句子进行表示。

**[26]. Transfer Reinforcement Learning using Output-­Gated Working Memory**
- 作者: Arthur Williams (Middle Tennessee State University)*; Joshua Phillips (Middle Tennessee State University)
- 简介：论文针对迁移强化学习，借鉴了监督学习任务中的工作memory输出门控神经网络模型的泛化能力，提出了一种迁移强化学习方法，取得了很好的效果

**[27]. Reinforcement-­Learning based Portfolio Management with Augmented Asset Movement Prediction States**
- 作者: Yunan Ye (Zhejiang University)*; Hengzhi Pei (Fudan University); Boxin Wang (University of Illinois at Urbana-­ Champaign); Pin-­Yu Chen (IBM Research); Yada Zhu (IBM Research); Jun Xiao (Zhejiang University); Bo Li (University of Illinois at Urbana–Champaign)
- 简介：为了解决PM问题中的数据异质性以及环境不确定性，论文提出一种SARL框架将利用多样性信息来进行趋势预测，并将这种趋势信息整合到RL算法中，作为状态增强

**[28]. Deep Reinforcement Learning for General Game Playing**
- 作者: Adrian Goldwaser (University of New South Wales)*; Michael Thielscher (University of New South Wales)
- 简介：这篇文章提到GGP的研究目前已经表现出一些潜力，例如alphaZero可以通过self-play同时学会多种棋类游戏，当前将qlearning结合进来还干不过UCT,本文就是将这个方向向前推动一步，可以干过UCT了，主要是将alphaZero进行了延伸，不仅仅局限于零和、双人对称博弈游戏

**[29]. Stealthy and Efficient Adversarial Attacks against Deep Reinforcement Learning**
- 作者: Jianwen Sun (Nanyang Technological University)*; Tianwei Zhang ( Nanyang Technological University); Xiaofei Xie (Nanyang Technological University); Lei Ma (Kyushu University); Yan Zheng (Tianjin University); Kangjie Chen (Tianjin University); Yang Liu (Nanyang Technology University, Singapore)
- 简介：DRL的对抗攻击研究很重要，有助于设计更加鲁棒的RL算法，研究目标是能够更加高效更加隐秘地进行攻击，主要提出两种攻击方式，可以在非常少的攻击步数下完成有效攻击，同时由于考虑了agent的最终目标，因此考虑了长期影响使得攻击更加有效。主要优势：第一能够进行全局最优攻击；第二能够具有通用性；第三考虑了长期伤害影响

**[30]. LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-­Based Games**
- 作者: Leonard Adolphs (ETHZ)*; Thomas Hofmann (ETH Zurich)
- 简介：论文针对微软的文本游戏比赛第二名agent进行了介绍，该游戏的主要挑战是部分可观测、大且稀疏的状态和行动空间，长期奖励分配，以及对于没见过的任务的泛化能力

**[31]. Induction of Subgoal Automata for Reinforcement Learning**
- 作者: Daniel Furelos-­Blanco (Imperial College London)*; Mark Law (Imperial College London); Alessandra Russo (Imperial College London); Krysia Broda (Imperial College London); Anders Jonsson (UPF)
- 简介：论文为了解决RL算法泛化和迁移能力的问题，指出抽象是主要途径，而层次RL是一种有希望的方法，提出了一种利用automata抽象的框架，同时区别于以往的工作，本文的automaton是在RL过程中学习出来的，最后通过归纳逻辑编程系统进行学习求解。

**[32]. MRI Reconstruction with Interpretable Pixel-­Wise Operations Using Reinforcement Learning**
- 作者: wentian li (Tsinghua University)*; XIDONG FENG (department of Automation,Tsinghua University); Haotian An (Tsinghua University); Xiang Yao Ng (Tsinghua University); Yu-­Jin Zhang (Tsinghua University)
- 简介：论文针对MRI重构的问题，为了解决可解释性，提出利用像素级别的RL算法来解决这个问题，处理过程更加透明

**[33]. Explainable Reinforcement Learning Through a Causal Lens**
- 作者: Prashan Madumal (University of Melbourne)*; Tim Miller (University of Melbourne); Liz Sonenberg (University of Melbourne); Frank Vetere (University of Melbourne)
- 简介：论文针对过去的可解释性RL研究缺乏对因果关系和反事实的关注，提出了一种构建行动对变量影响关系模型，提出用因果模型来生成why和why not的对比解释

**[34]. Reinforcement Learning based Metapath Discovery in Large-­scale Heterogeneous Information Networks**
- 作者: Guojia Wan (Wuhan University); Bo Du (School of Compuer Science, Wuhan University)*; Shirui Pan (Monash University); Reza Haffari (Monash University, Australia)
- 简介：提出了一种自动发现元路径的RL算法，一方面不需要预训练，第二可以构建变长的元路径；第三可以考虑多种节点类型，适用于大规模，在链接预测任务上表现更好

**[35]. Reinforcement Learning When All Actions are Not Always Available**
- 作者: Yash Chandak (University of Massachusetts Amherst)*; Georgios Theocharous ("Adobe Research, USA"); Blossom Metevier (University of Massachusetts, Amherst); Philip Thomas (University of Massachusetts Amherst)
- 简介：论文针对行动集合具有随机性的问题，提出基于Q-learning的方法收敛性问题更加严重，进而提出了基于策略梯度的方法和自然策略梯度的方法，同时给出了接近收敛的条件，同时为了削减方差也进一步提出了相关的技术

**[36]. Reinforcement Mechanism Design: With Applications to Dynamic Pricing in Sponsored Search Auctions**
- 作者: Weiran Shen (Carnegie Mellon University)*; Binghui Peng (Columbia University); Hanpeng Liu (Tsinghua University); Michael Zhang (Chinese University of Hong Kong); Ruohan Qian (Baidu Inc.); Yan Hong (Baidu Inc.); Zhi Guo (Baidu Inc.); Zongyao Ding (Baidu Inc.); Pengjun Lu (Baidu Inc.); Pingzhong Tang (Tsinghua University)
- 简介：论文关注与付费搜索领域的竞价机制研究，指出最优收益并不一定需要按照竞价来排序位置，最优收益应该跟相关广告商的底价向量更相关

**[37]. Metareasoning in Modular Software Systems: On-­the-­Fly Configuration Using Reinforcement Learning**
- 作者: Rich Contextual Representations Aditya Modi (Univ. of Michigan Ann Arbor)*; Debadeepta Dey (Microsoft); Alekh Agarwal (Microsoft); Adith Swaminathan (Microsoft Research); Besmira Nushi (Microsoft Research); Sean Andrist (Microsoft Research); Eric Horvitz (MSR)
- 简介：针对大型模块化软件参数配置调优的问题，主要考虑动态性和个性化的问题，提出一种基于元推理的适应性运行中强化学习动态配置算法。

**[38]. Joint Entity and Relation Extraction with a Hybrid Transformer and Reinforcement Learning Based Model**
- 作者: Ya Xiao (Tongji University)*; Chengxiang Tan (Tongji University); Zhijie Fan (The Third Research Institute of the Ministry of Public Security); Qian Xu (Tongji University); Wenye Zhu (Tongji University)
- 简介：论文主要要解决的就是联合抽取同时考虑噪声数据过滤的问题，提出一种混合模型来解决这个问题，具体的联合抽取通过参数共享和子任务交互来解决，噪声过滤主要通过RL来解决

**[39]. Reinforcement Learning of Risk-­Constrained Policies in Markov Decision Processes**
- 作者: Tomas Brazdil (Masaryk University); Krishnendu Chatterjee (IST Austria); Petr Novotný (Masaryk University)*; Jiří Vahala (Masaryk University)
- 简介：论文针对风险约束的规划问题展开研究，虽然已经有人研究过这个问题，但是他们只考虑的是确定性策略，没有考虑随机策略或混合策略，严格来讲这两种应该是更好的策略。最直接的方法是利用线性规划或者动态规划来解决，但是都存在scale问题，所以论文主要提出了一种算法，结合了UCT-like搜索以及一个风险约束的行动选择，这个行动选择在一个规模更小的子问题上，因此可以用线性规划来解决。虽然预测器的引入导致方法丧失了guarantee，但是还是有很好的性能表现的

**[40]. Deep Model-­Based Reinforcement Learning via Estimated Uncertainty and Conservative Policy Optimization**
- 作者: Qi Zhou (University of Science and Technology of China); Houqiang Li (University of Science and Technology of China); Jie Wang (University of Science and Technology of China)*
- 简介：论文针对基于模型的方法存在的过拟合模型偏差的问题，提出一种基于Qvalue的不确定性上界评估算法，以及基于性能提升概率评估的保守策略优化算法，两者结合可以达到sota性能，并具有极好的鲁棒性

**[41]. Reinforcement Learning with Non-­Markovian Rewards**
- 作者: Maor Gaon (Ben-­Gurion University); Ronen Brafman (BGU)*
- 简介：论文针对非马尔可夫奖励问题，提出一种在无需已知环境模型的前提下的RL算法，本质上的思路还是对状态进行扩展，增加能够描述历史的状态变量，但是过去的方法还需要知道状态模型和奖励结构，这在RL问题中经常是不知道的。所以结合了automata来解决这个问题。

**[42]. Modular Robot Design Synthesis with Deep Reinforcement Learning**
- 作者: Julian Whitman (Carnegie Mellon University)*; Raunaq Bhirangi (Carnegie Mellon University); Matthew Travers (CMU); Howie Choset (Carnegie Melon University)
- 简介：论文将机器人组装问题构建成一个MDP问题，然后在DQN框架下学习添加模块动作的值函数，并将值函数当做best-first图搜索的启发式信息，取得了超过其他相关方法的效果。

**[43]. BAR -­A Reinforcement Learning Agent for Bounding-­Box Automated Refinement**
- 作者: Morgane Ayle (American University of Beirut -­ AUB)*; Jimmy Tekli (BMW Group / Université de Franche-­Comté -­ UFC); Julia Zini (American University of Beirut -­ AUB); Boulos El Asmar (BMW Group / Karlsruher Institut für Technologie -­ KIT); Mariette Awad (American University of Beirut-­ AUB)

**[44]. Hierarchical Reinforcement Learning for Open-­Domain Dialog**
- 作者:  Abdelrhman Saleh (Harvard University)*; Natasha Jaques (MIT); Asma Ghandeharioun (MIT); Judy Hanwen Shen(MIT); Rosalind Picard (MIT Media Lab)
- 简介：本文针对开域对话问题的挑战，包括内容没有保障、最大似然估计训练结果总是重复、无法建立对历史对话的跟踪和利用，提出一种变分层次强化学习方法，通过实验表明具有超过transformer模型的效果

**[45]. Copy or Rewrite: Hybrid Summarization with Hierarchical Reinforcement Learning**
- 作者: Liqiang Xiao (Artificial Intelligence Institute, SJTU)*; Lu Wang (Khoury College of Computer Science, Northeastern University); Hao He (Shanghai Jiao Tong University); Yaohui Jin (Artificial Intelligence Institute, SJTU)
- 简介：本文针对摘要问题进行研究，虽然已经有一些方法已经考虑将提取和摘要进行组合，但是目前仍然存在摘要阶段的信息丢失问题，因为没有将句子进行有效区分，因此本文提出一种能够更好地将两者结合的方式，让两个子任务在动态训练过程中相互合作、适应，从而提升效果

**[46]. Generalizable Resource Allocation in Stream Processing via Deep Reinforcement Learning**
- 作者: Xiang Ni (IBM Research); Jing Li (NJIT); Wang Zhou (IBM Research); Mo Yu (IBM T. J. Watson)*; Kun-­Lung Wu (IBM Research)
- 简介：论文针对流处理系统中的分布式资源分配问题，提出一种利用DRL来进行图划分的方法，同时也提出了一种图感知的状态表示方法，最后还提出了新的benchmark和评价方法

**[47]. Actor Critic Deep Reinforcement Learning for Neural Malware Control**
- 作者: Yu Wang (Microsoft)*; Jack Stokes (Microsoft Research); Mady Marinescu (Microsoft Corporation)
- 简介：论文提出一种修改的AC算法来解决先前的用于恶意软件检测的RL算法无法考虑对抗攻击的问题，能够将性能进一步有所提升

**[48]. Fixed-­Horizon Temporal Difference Methods for Stable Reinforcement Learning**
- 作者: Kristopher De Asis (University of Alberta)*; Alan Chan (University of Alberta); Silviu Pitis (University of Toronto); Richard Sutton (University of Alberta); Daniel Graves (Huawei)
- 简介：本文为了得到稳定的强化学习算法，提出了固定视野的TD算法，通过综合多个视野下的结果来预测，取得更好的结果

**[49]. Sequence Generation with Optimal-­Transport-­Enhanced Reinforcement Learning**
- 作者: Liqun Chen (Duke University)*; Ke Bai (Duke University); Chenyang Tao (Duke University); Yizhe Zhang (Microsoft Research); Guoyin Wang (Duke University); Wenlin Wang (Duke Univeristy); Ricardo Henao (Duke University); Lawrence Carin Duke (CS)
- 简介：问题背景是自然语言序列生成，当前的现状是利用RL效果不错，其中要点是奖励函数设计，主要有两类，静态奖励和动态奖励，静态奖励是一种人类代理的近似，显然存在偏差，动态奖励目前主要用对抗学习，但是存在模式崩塌或梯度消失的问题，OT学习是最近比较火的一种技术，可以表示为一种代价依赖的离散分布匹配问题，本文将RL和OT进行了有效的结合，取得了更好的结果。

**[50]. Scaling All-­Goals Updates in Reinforcement Learning Using Convolutional Neural Networks**
- 作者: Fabio Pardo (Imperial College London)*; Vitaly Levdik (Imperial College London); Petar Kormushev (Imperial College London)
- 简介：针对有多个目标的问题，提出一种利用卷积网络提高性能的方法。

**[51]. Parameterized Indexed Value Function for Efficient Exploration in Reinforcement Learning**
- 作者: Tian Tan (Stanford University)*; Zhihan Xiong (Stanford University); Vikranth Dwaracherla (Stanford University)
- 简介：针对RL中的高效探索采样问题，过去的随机值函数方法虽然可行但是仍然需要大量的计算资源，为此本文利用index sampling来提出一种计算高效的基于不确定性行动值函数的方法，并提出 一种算法来学习。另外，从计算的角度，提出一种对偶网络架构，PIN，包含一个mean network和一个uncertainty network来学习indexed value function。

**[52]. Solving Online Threat Screening Games using Constrained Action Space Reinforcement Learning**
- 作者: Sanket Shah (Singpore Management University)*; Arunesh Sinha (Singapore Management University); Pradeep Varakantham (Singapore Management University); Andrew Perrault (Harvard University); Milind Tambe (Harvard University)
